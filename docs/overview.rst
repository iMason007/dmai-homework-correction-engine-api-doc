************************************************
概念和名词介绍
************************************************

.. contents:: 目录

本页内容并非严谨学术表达，只是为了方便理解本套代码的模型训练过程的简单介绍。

人工神经网络
======================================

机器学习领域中，人工神经网络是一种模仿生物神经网络的结构和功能的计算模型，用于\
对函数进行估计或近似，一般由大量神经元组成，具备学习功能。

人工神经网络包括结构、激励函数、学习规则三个部分，其中，结构和激励函数决定了网\
络如何把数据输入进行计算得到输出，而学习规则则指定了网络中的权重/参数如何随时间\
调整，拟合学习目标。

人工神经网络的一步训练过程以一个神经元为例如下：

1. 前向传播：神经元内部参数和输入进行内积计算并经非线性激励函数获得输出值；

2. 反向传播：通过计算网络中所有参数关于损失 (loss) 函数的梯度；

3. 最优化：用梯度下降法更新网络参数，从而最小化损失函数 (让网络输出值和目标输出值更接近)。

这种学习方法叫反向传播算法。训练一个神经网络的过程就是反复重复以上的步骤，直到模型收敛。

代码中与神经网络训练相关的主要名称和意义如下：


+---------------+----------+--------------------------------------------+
|     名称      |  中文名  |                    说明                    |
+===============+==========+============================================+
| loss          | 损失     | 损失函数的值，代表预测值和目标值的差别     |
+---------------+----------+--------------------------------------------+
| step          | 步       | 每次更新网络参数可以看作 1 步              |
+---------------+----------+--------------------------------------------+
| forward       | 前向传播 | 数据输入网络计算得到输出值                 |
+---------------+----------+--------------------------------------------+
| backward      | 反向传播 | 计算网络中所有参数关于损失(loss)函数的梯度 |
+---------------+----------+--------------------------------------------+
| learning rate | 学习率   | 每次梯度下降更新网络参数的幅度             |
+---------------+----------+--------------------------------------------+


强化学习
========

机器学习中，强化学习强调如何基于环境而行动，以取得最大化的预期收益。其灵感来源\
于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成\
对刺激的预期，产生能获得最大利益的习惯性行为。

.. image:: _static/RL.jpg

强化学习的过程是：智能体 (agent) 在某个时间步 (time step) 根据对环境的观测信息 (observation) 在\
环境中做了一个动作 (action)，转移到下一个状态 (state)，并得到环境的反馈——一个标量形式的回\
报 (reward)，智能体根据反馈调整自己的策略 (policy)，目标是让自己在环境中的累积回报 (return) 最大化。

本算法框架选用的 :ref:`D3QN <训练算法 (Dueling Double Deep Q Network (D3QN))>` 强化学习算法是基于值 (value-based) 的强化学习算法，其目标\
在于利用神经网络对当前状态下可选动作 (在强化学习中以 action 表示，在本框架下为目\
标 goal) 的未来预期收益——价值 (value) 进行预测，并选择价值最大的动作。

代码中与强化学习相关的主要名称和意义如下表所示：

+---------+----------+-------------------------------------------------------------------------------------------------+
|  名称   |  中文名  |                                              说明                                               |
+=========+==========+=================================================================================================+
| env     | 环境     | 智能体要学习在其中获得最大化预期收益的环境                                                      |
+---------+----------+-------------------------------------------------------------------------------------------------+
| policy  | 策略     | 从环境观测信息到动作选择的映射                                                                  |
+---------+----------+-------------------------------------------------------------------------------------------------+
| agent   | 智能体   | 指算法可控制其与环境进行交互的个体，在游戏中就是玩家                                            |
+---------+----------+-------------------------------------------------------------------------------------------------+
| episode | 回合     | 一个包含多个时间步的、从起始状态到结束的过程，强化学习的目标就是最大化这段时间内的累积回报      |
+---------+----------+-------------------------------------------------------------------------------------------------+
| state   | 状态     | 特定时间时智能体所处的状态                                                                      |
+---------+----------+-------------------------------------------------------------------------------------------------+
| obs     | 观测信息 | 某个时间步智能体从环境获取的信息                                                                |
+---------+----------+-------------------------------------------------------------------------------------------------+
| goal    | 目标     | 可以尝试通过一系列条件和动作达成的目标，在本算法框架中的地位近似等价于强化学习中的动作 (action) |
+---------+----------+-------------------------------------------------------------------------------------------------+
| action  | 动作     | 1. 一般在强化学习中，表示当前时间步，智能体可以在环境中采取执行并获得回报的动作                 |
|         |          | 2. 本算法框架中，表示执行目标过程中，分解出可直接在环境中作出的动作                             |
+---------+----------+-------------------------------------------------------------------------------------------------+
| effect  | 增益     | 各状态值在两个状态之间的差值以及两个状态间额外计算的变化量，多维向量形式                        |
+---------+----------+-------------------------------------------------------------------------------------------------+
| reward  | 回报     | 环境给智能体反馈的标量值，可能包含奖励 (正值) 和惩罚 (负值)                                     |
+---------+----------+-------------------------------------------------------------------------------------------------+
| done    | 结束信号 | 回合结束的信号，可能是达到设定的最大步数，或智能体死亡等条件触发                                |
+---------+----------+-------------------------------------------------------------------------------------------------+
| value   | 价值     | 对状态/动作评估值，其中                                                                         |
|         |          |                                                                                                 |
|         |          | - 状态价值：当前状态下估计未来的累积回报值                                                      |
|         |          |                                                                                                 |
|         |          | - 动作价值：当前状态下执行某动作，估计未来的累积回报值                                          |
+---------+----------+-------------------------------------------------------------------------------------------------+